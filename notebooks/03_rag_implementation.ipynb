{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCG Multi-Agent & Multimodal AI Platform - RAG Implementation\n",
    "\n",
    "This notebook demonstrates the Retrieval-Augmented Generation (RAG) components of the BCG Multi-Agent & Multimodal AI Platform, including:\n",
    "1. Setting up advanced retrievers for document retrieval\n",
    "2. Building effective prompts for the LLM\n",
    "3. Generating accurate responses using the RAG pipeline\n",
    "4. Testing various RAG use cases with BCG Sustainability Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from configs.config import (\n",
    "    RAW_DATA_DIR,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    EMBEDDINGS_DIR,\n",
    "    VECTOR_STORE_PATH,\n",
    "    LLM_MODEL,\n",
    "    OLLAMA_BASE_URL,\n",
    ")\n",
    "\n",
    "# Check if we have the necessary directories and vector store\n",
    "print(f\"Vector store path: {VECTOR_STORE_PATH}\")\n",
    "vector_store_exists = VECTOR_STORE_PATH.exists() and any(VECTOR_STORE_PATH.iterdir())\n",
    "print(f\"Vector store exists: {vector_store_exists}\")\n",
    "\n",
    "# Check if Ollama is available\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Ollama is available at {OLLAMA_BASE_URL}\")\n",
    "        models = response.json().get(\"models\", [])\n",
    "        if models:\n",
    "            print(f\"Available models: {[model['name'] for model in models]}\")\n",
    "        else:\n",
    "            print(\"No models found in Ollama\")\n",
    "    else:\n",
    "        print(f\"Ollama API returned status code {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {str(e)}\")\n",
    "    print(f\"Please ensure Ollama is running at {OLLAMA_BASE_URL} with the {LLM_MODEL} model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Embeddings Manager\n",
    "\n",
    "First, we need to set up the embeddings manager that will handle our vector store operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.embeddings.embeddings_manager import EmbeddingsManager\n",
    "\n",
    "# Initialize embeddings manager\n",
    "embeddings_manager = EmbeddingsManager(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_type=\"local\",\n",
    "    vector_store_dir=str(VECTOR_STORE_PATH),\n",
    "    collection_name=\"bcg_sustainability_reports\",\n",
    "    cache_folder=str(EMBEDDINGS_DIR / \"models\"),\n",
    ")\n",
    "\n",
    "# Get vector store statistics\n",
    "stats = embeddings_manager.get_statistics()\n",
    "print(\"Embeddings Manager Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\" - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Document Retrieval\n",
    "\n",
    "Let's test the document retrieval capability to ensure we can fetch relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test simple retrieval\n",
    "query = \"What are BCG's climate commitments?\"\n",
    "search_results = embeddings_manager.search(query, k=3, use_mmr=True)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Retrieved {len(search_results)} documents\\n\")\n",
    "\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\" if len(doc.page_content) > 300 else f\"Content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Response Generator\n",
    "\n",
    "Now, let's set up the response generator that will use the LLM to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.rag.generator import ResponseGenerator\n",
    "\n",
    "# Initialize the response generator using Ollama\n",
    "response_generator = ResponseGenerator(\n",
    "    model_name=LLM_MODEL,\n",
    "    model_type=\"ollama\",\n",
    "    temperature=0.2,\n",
    "    streaming=True,  # Set to True to see real-time generation\n",
    "    ollama_base_url=OLLAMA_BASE_URL,\n",
    ")\n",
    "\n",
    "# Test the response generator with a simple prompt\n",
    "test_prompt = \"\"\"You are an AI assistant specialized in analyzing BCG Sustainability Reports.\n",
    "Please give a brief introduction of yourself in 3 sentences.\"\"\"\n",
    "\n",
    "print(\"Testing response generator...\\n\")\n",
    "response = response_generator.generate_response(test_prompt)\n",
    "print(\"\\nResponse:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Prompt Builder\n",
    "\n",
    "Let's test the prompt builder to see how it structures prompts for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.rag.prompt_builder import PromptBuilder\n",
    "\n",
    "# Initialize the prompt builder\n",
    "prompt_builder = PromptBuilder(\n",
    "    include_source_documents=True,\n",
    "    max_context_length=8000,\n",
    ")\n",
    "\n",
    "# Build a RAG prompt using the retrieved documents\n",
    "query = \"What are BCG's commitments to achieve net zero?\"\n",
    "search_results = embeddings_manager.search(query, k=3, use_mmr=True)\n",
    "\n",
    "rag_prompt = prompt_builder.build_rag_prompt(\n",
    "    query=query,\n",
    "    documents=search_results,\n",
    ")\n",
    "\n",
    "print(\"RAG Prompt Preview:\")\n",
    "print(rag_prompt.messages[0].content[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Enhanced Retriever\n",
    "\n",
    "Now let's set up the enhanced retriever and test its retrieval capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.rag.retriever import EnhancedRetriever, MultiQueryRetriever\n",
    "\n",
    "# Create an enhanced retriever\n",
    "enhanced_retriever = EnhancedRetriever(\n",
    "    embeddings_manager=embeddings_manager,\n",
    "    search_kwargs={\"k\": 3},\n",
    "    use_mmr=True,\n",
    "    fetch_k=10,\n",
    "    lambda_mult=0.7,\n",
    ")\n",
    "\n",
    "# Test the enhanced retriever\n",
    "query = \"What progress has BCG made in reducing its carbon emissions?\"\n",
    "docs = enhanced_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Enhanced retriever found {len(docs)} documents for query: '{query}'\\n\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\" if len(doc.page_content) > 300 else f\"Content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Multi-Query Retriever\n",
    "\n",
    "Let's test the multi-query retriever, which generates variations of the original query to improve recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a multi-query retriever\n",
    "multi_query_retriever = MultiQueryRetriever(\n",
    "    base_retriever=enhanced_retriever,\n",
    "    llm=response_generator.get_llm(),\n",
    "    num_queries=3,\n",
    ")\n",
    "\n",
    "# Test the multi-query retriever\n",
    "query = \"How does BCG support diversity and inclusion?\"\n",
    "docs = multi_query_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Multi-query retriever found {len(docs)} documents for query: '{query}'\\n\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "    print(f\"Content: {doc.page_content[:300]}...\" if len(doc.page_content) > 300 else f\"Content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Complete RAG Pipeline\n",
    "\n",
    "Now let's test the complete RAG pipeline, which combines retrieval, prompt building, and response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.rag.rag_pipeline import RAGPipeline\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag_pipeline = RAGPipeline(\n",
    "    embeddings_manager=embeddings_manager,\n",
    "    response_generator=response_generator,\n",
    "    retriever_type=\"enhanced\",\n",
    "    use_multi_query=False,  # Start with simple retrieval\n",
    "    include_sources=True,\n",
    "    max_sources=4,\n",
    ")\n",
    "\n",
    "# Test the RAG pipeline with a query\n",
    "query = \"What are BCG's key achievements in sustainability over the last three years?\"\n",
    "response = rag_pipeline.query(query)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Response:\")\n",
    "print(response.response)\n",
    "\n",
    "print(\"\\nSource Documents:\")\n",
    "for i, doc in enumerate(response.source_documents[:3]):  # Show first 3 sources\n",
    "    print(f\"Source {i+1}: {doc.metadata.get('filename', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare RAG Pipeline with Different Retrievers\n",
    "\n",
    "Let's compare the performance of the RAG pipeline with different retriever configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the RAG pipeline with multi-query retriever\n",
    "multi_query_rag_pipeline = RAGPipeline(\n",
    "    embeddings_manager=embeddings_manager,\n",
    "    response_generator=response_generator,\n",
    "    retriever_type=\"enhanced\",\n",
    "    use_multi_query=True,  # Enable multi-query retrieval\n",
    "    include_sources=True,\n",
    "    max_sources=4,\n",
    ")\n",
    "\n",
    "# Test the multi-query RAG pipeline with the same query\n",
    "query = \"What specific steps is BCG taking to reduce scope 3 emissions?\"\n",
    "\n",
    "print(\"Testing standard RAG pipeline...\")\n",
    "standard_response = rag_pipeline.query(query)\n",
    "\n",
    "print(\"\\nTesting multi-query RAG pipeline...\")\n",
    "multi_query_response = multi_query_rag_pipeline.query(query)\n",
    "\n",
    "print(\"\\n--- Standard RAG Response ---\")\n",
    "print(standard_response.response)\n",
    "\n",
    "print(\"\\n--- Multi-Query RAG Response ---\")\n",
    "print(multi_query_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Comparisons Across Years\n",
    "\n",
    "Let's test the RAG pipeline's ability to compare information across different BCG sustainability reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test yearly comparison\n",
    "topic = \"carbon emission reduction targets\"\n",
    "comparison_response = rag_pipeline.compare_across_years(topic)\n",
    "\n",
    "print(f\"Comparison of '{topic}' across years:\\n\")\n",
    "print(comparison_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Extract Key Metrics and Facts\n",
    "\n",
    "Let's test the RAG pipeline's ability to extract key metrics and facts from the BCG sustainability reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract metrics\n",
    "metrics_response = rag_pipeline.extract_facts(fact_type=\"metrics\")\n",
    "\n",
    "print(\"Extracted Metrics:\\n\")\n",
    "print(metrics_response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract commitments\n",
    "commitments_response = rag_pipeline.extract_facts(fact_type=\"commitments\")\n",
    "    \n",
    "print(\"Extracted Commitments:\\n\")\n",
    "print(commitments_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate a Comprehensive Summary\n",
    "\n",
    "Let's test the RAG pipeline's ability to generate a comprehensive summary of the BCG sustainability reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a general summary\n",
    "summary_response = rag_pipeline.generate_summary()\n",
    "\n",
    "print(\"Summary of BCG Sustainability Reports:\\n\")\n",
    "print(summary_response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate a focused summary on a specific topic\n",
    "topic = \"diversity, equity, and inclusion\"\n",
    "focused_summary_response = rag_pipeline.generate_summary(topic=topic)\n",
    "\n",
    "print(f\"Summary of BCG's {topic} initiatives:\\n\")\n",
    "print(focused_summary_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test RAG Pipeline with Specific Filters\n",
    "\n",
    "Let's test the RAG pipeline with specific filters to focus on particular documents or years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test filter by document (assuming we have multiple documents)\n",
    "import re\n",
    "\n",
    "# Find documents from specific years\n",
    "query = \"BCG's climate and sustainability initiatives\"\n",
    "all_docs = embeddings_manager.search(query, k=10)\n",
    "\n",
    "# Extract unique document IDs\n",
    "doc_ids = set()\n",
    "for doc in all_docs:\n",
    "    if \"document_id\" in doc.metadata:\n",
    "        doc_ids.add(doc.metadata[\"document_id\"])\n",
    "\n",
    "print(f\"Found documents with IDs: {doc_ids}\\n\")\n",
    "\n",
    "# If we have at least one document ID, test filtering\n",
    "if doc_ids:\n",
    "    doc_id = list(doc_ids)[0]\n",
    "    print(f\"Testing RAG pipeline with filter for document ID: {doc_id}\")\n",
    "    \n",
    "    filter_criteria = {\"document_id\": doc_id}\n",
    "    filtered_response = rag_pipeline.query(\n",
    "        query=\"What are BCG's commitments to sustainability?\",\n",
    "        filter_criteria=filter_criteria,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFiltered Response:\")\n",
    "    print(filtered_response.response)\n",
    "else:\n",
    "    print(\"No document IDs found for filtering test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Complex Queries\n",
    "\n",
    "Let's test the RAG pipeline with more complex, multi-part queries to evaluate its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test a complex query\n",
    "complex_query = \"\"\"How has BCG's approach to climate action evolved over the last three years, \n",
    "and what specific achievements or initiatives have they implemented? \n",
    "Also, how does this compare to their diversity and inclusion efforts during the same period?\"\"\"\n",
    "\n",
    "complex_response = rag_pipeline.query(complex_query)\n",
    "\n",
    "print(f\"Complex Query: '{complex_query}'\\n\")\n",
    "print(\"Response:\")\n",
    "print(complex_response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Evaluation of RAG Performance\n",
    "\n",
    "Let's evaluate the performance of our RAG pipeline with some basic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a set of test queries\n",
    "test_queries = [\n",
    "    \"What are BCG's carbon emission reduction targets?\",\n",
    "    \"How does BCG approach diversity and inclusion?\",\n",
    "    \"What community engagement initiatives does BCG participate in?\",\n",
    "    \"How is BCG supporting climate action with its clients?\",\n",
    "    \"What is BCG's strategy for responsible business practices?\"\n",
    "]\n",
    "\n",
    "# Set up a simple evaluation function\n",
    "def evaluate_response(query, response):\n",
    "    \"\"\"Simple evaluation function to assess response quality.\"\"\"\n",
    "    # Check if sources are cited\n",
    "    has_citations = \"[Document\" in response and \"]\" in response\n",
    "    \n",
    "    # Check for common hallucination indicators\n",
    "    hallucination_indicators = [\n",
    "        \"I don't have enough information\",\n",
    "        \"not mentioned in the provided\",\n",
    "        \"cannot provide specific\",\n",
    "        \"don't have access to\"\n",
    "    ]\n",
    "    potential_hallucination = any(indicator in response for indicator in hallucination_indicators)\n",
    "    \n",
    "    # Check response length as a basic measure\n",
    "    word_count = len(response.split())\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"has_citations\": has_citations,\n",
    "        \"potential_hallucination\": potential_hallucination,\n",
    "        \"word_count\": word_count,\n",
    "    }\n",
    "\n",
    "# Run evaluations\n",
    "evaluation_results = []\n",
    "for query in test_queries:\n",
    "    print(f\"Testing query: '{query}'\")\n",
    "    response_obj = rag_pipeline.query(query)\n",
    "    result = evaluate_response(query, response_obj.response)\n",
    "    result[\"source_count\"] = len(response_obj.source_documents)\n",
    "    evaluation_results.append(result)\n",
    "    print(f\"Result: {len(response_obj.source_documents)} sources, {result['word_count']} words\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Display summary\n",
    "import pandas as pd\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "display(eval_df)\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"Average word count: {eval_df['word_count'].mean():.1f}\")\n",
    "print(f\"Percentage with citations: {(eval_df['has_citations'].sum() / len(eval_df) * 100):.1f}%\")\n",
    "print(f\"Percentage with potential hallucinations: {(eval_df['potential_hallucination'].sum() / len(eval_df) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Conclusion\n",
    "\n",
    "In this notebook, we've successfully demonstrated the RAG components of the BCG Multi-Agent & Multimodal AI Platform. We've shown how the system can retrieve relevant information from BCG Sustainability Reports and generate accurate, contextually relevant responses to various queries.\n",
    "\n",
    "Key accomplishments include:\n",
    "1. Setting up enhanced retrievers for effective document retrieval\n",
    "2. Building structured prompts for the LLM\n",
    "3. Generating accurate responses using the complete RAG pipeline\n",
    "4. Testing various RAG use cases including comparisons, summaries, and fact extraction\n",
    "5. Evaluating the performance of the RAG system\n",
    "\n",
    "These components form the foundation of our multi-agent system, which will be implemented in the next phase of the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}