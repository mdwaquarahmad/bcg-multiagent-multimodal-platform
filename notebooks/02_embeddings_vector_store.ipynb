{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCG Multi-Agent & Multimodal AI Platform - Embeddings and Vector Store\n",
    "\n",
    "This notebook demonstrates the embeddings generation and vector store components of the BCG Multi-Agent & Multimodal AI Platform, including:\n",
    "1. Generating embeddings for document chunks\n",
    "2. Storing embeddings in a vector database\n",
    "3. Performing semantic search queries\n",
    "4. Analyzing search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from configs.config import (\n",
    "    RAW_DATA_DIR,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    EMBEDDINGS_DIR,\n",
    "    VECTOR_STORE_PATH,\n",
    ")\n",
    "\n",
    "# Ensure the embeddings directory exists\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_STORE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"Embeddings directory: {EMBEDDINGS_DIR}\")\n",
    "print(f\"Vector store path: {VECTOR_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check for Processed Documents\n",
    "\n",
    "Let's check if we have processed documents available from the previous data ingestion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Find processed document directories\n",
    "processed_dirs = [d for d in PROCESSED_DATA_DIR.iterdir() if d.is_dir()]\n",
    "\n",
    "# Collect document info\n",
    "document_info = []\n",
    "\n",
    "for doc_dir in processed_dirs:\n",
    "    json_files = list(doc_dir.glob(\"*.json\"))\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_data = json.load(f)\n",
    "            \n",
    "            document_info.append({\n",
    "                \"document_id\": doc_data.get(\"document_id\", \"\"),\n",
    "                \"filename\": doc_data.get(\"filename\", \"\"),\n",
    "                \"chunks\": len(doc_data.get(\"text_chunks\", [])),\n",
    "                \"visuals\": len(doc_data.get(\"visual_elements\", [])),\n",
    "                \"json_path\": str(json_file),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {json_file}: {str(e)}\")\n",
    "\n",
    "if document_info:\n",
    "    print(f\"Found {len(document_info)} processed documents:\")\n",
    "    for doc in document_info:\n",
    "        print(f\" - {doc['filename']}: {doc['chunks']} chunks, {doc['visuals']} visuals\")\n",
    "else:\n",
    "    print(\"No processed documents found. Please run the data ingestion notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Embedding Generator\n",
    "\n",
    "Let's test the embedding generator with some sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.embeddings.embedding_generator import EmbeddingGenerator\n",
    "\n",
    "# Initialize embedding generator with a local model\n",
    "embedding_generator = EmbeddingGenerator(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Small but effective model\n",
    "    model_type=\"local\",\n",
    "    cache_folder=str(EMBEDDINGS_DIR / \"models\"),\n",
    ")\n",
    "\n",
    "# Test with some sample text\n",
    "sample_texts = [\n",
    "    \"BCG is committed to reducing carbon emissions and achieving net zero.\",\n",
    "    \"Sustainability is a core value of Boston Consulting Group's strategy.\",\n",
    "    \"Digital transformation is revolutionizing business operations.\",\n",
    "    \"Artificial intelligence and machine learning drive innovation.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for sample texts\n",
    "embeddings = embedding_generator.generate_embeddings(sample_texts)\n",
    "\n",
    "# Display information about the embeddings\n",
    "embedding_dimension = embedding_generator.get_embedding_dimension()\n",
    "print(f\"Embedding dimension: {embedding_dimension}\")\n",
    "print(f\"Generated {len(embeddings)} embeddings with dimension {len(embeddings[0])}\")\n",
    "\n",
    "# Show a sample of the first embedding\n",
    "print(\"\\nSample of first embedding vector:\")\n",
    "print(embeddings[0][:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Vector Store\n",
    "\n",
    "Let's test the vector store by adding some sample documents and performing searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.embeddings.vector_store import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize vector store with the embedding model\n",
    "vector_store = VectorStore(\n",
    "    embedding_model=embedding_generator.get_embedding_model(),\n",
    "    persist_directory=str(VECTOR_STORE_PATH / \"test\"),\n",
    "    collection_name=\"test_collection\",\n",
    ")\n",
    "\n",
    "# Create sample documents with metadata\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=sample_texts[0],\n",
    "        metadata={\n",
    "            \"document_id\": \"doc_1\",\n",
    "            \"topic\": \"sustainability\",\n",
    "            \"year\": 2023,\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=sample_texts[1],\n",
    "        metadata={\n",
    "            \"document_id\": \"doc_1\",\n",
    "            \"topic\": \"sustainability\",\n",
    "            \"year\": 2023,\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=sample_texts[2],\n",
    "        metadata={\n",
    "            \"document_id\": \"doc_2\",\n",
    "            \"topic\": \"digital\",\n",
    "            \"year\": 2022,\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=sample_texts[3],\n",
    "        metadata={\n",
    "            \"document_id\": \"doc_2\",\n",
    "            \"topic\": \"digital\",\n",
    "            \"year\": 2022,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Add documents to vector store\n",
    "ids = vector_store.add_documents(sample_documents)\n",
    "vector_store.persist()\n",
    "\n",
    "print(f\"Added {len(ids)} documents to vector store\")\n",
    "print(f\"Document IDs: {ids}\")\n",
    "print(f\"Vector store now contains {vector_store.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Vector Store Search\n",
    "\n",
    "Let's test searching the vector store with different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test semantic search\n",
    "query = \"Environmental sustainability and carbon footprint reduction\"\n",
    "print(f\"Searching for: '{query}'\\n\")\n",
    "\n",
    "results = vector_store.search(query, k=2)\n",
    "\n",
    "print(\"Search Results:\")\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"Result {i+1} (Score: {score:.4f})\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test search with metadata filter\n",
    "query = \"business strategy\"\n",
    "filter_condition = {\"topic\": \"digital\"}\n",
    "print(f\"Searching for: '{query}' with filter: {filter_condition}\\n\")\n",
    "\n",
    "filtered_results = vector_store.search(query, k=2, filter=filter_condition)\n",
    "\n",
    "print(\"Filtered Search Results:\")\n",
    "for i, (doc, score) in enumerate(filtered_results):\n",
    "    print(f\"Result {i+1} (Score: {score:.4f})\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test MMR search for diverse results\n",
    "query = \"BCG business\"\n",
    "print(f\"Performing MMR search for: '{query}'\\n\")\n",
    "\n",
    "mmr_results = vector_store.search_mmr(query, k=3, lambda_mult=0.7)\n",
    "\n",
    "print(\"MMR Search Results:\")\n",
    "for i, doc in enumerate(mmr_results):\n",
    "    print(f\"Result {i+1}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Embeddings Manager\n",
    "\n",
    "Now let's test the embeddings manager that integrates embedding generation and vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.embeddings.embeddings_manager import EmbeddingsManager\n",
    "\n",
    "# Initialize embeddings manager\n",
    "embeddings_manager = EmbeddingsManager(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_type=\"local\",\n",
    "    vector_store_dir=str(VECTOR_STORE_PATH / \"bcg_docs\"),\n",
    "    collection_name=\"bcg_sustainability_reports\",\n",
    "    cache_folder=str(EMBEDDINGS_DIR / \"models\"),\n",
    ")\n",
    "\n",
    "# Get vector store statistics\n",
    "stats = embeddings_manager.get_statistics()\n",
    "print(\"Embeddings Manager Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\" - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process Processed Documents\n",
    "\n",
    "Now let's process our previously processed documents and add them to the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process all available documents\n",
    "if document_info:\n",
    "    print(f\"Processing {len(document_info)} documents...\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(document_info):\n",
    "        print(f\"Processing document {i+1}/{len(document_info)}: {doc['filename']}\")\n",
    "        \n",
    "        try:\n",
    "            # Process the JSON document\n",
    "            ids = embeddings_manager.process_json_document(doc['json_path'])\n",
    "            print(f\" - Added {len(ids)} chunks to vector store\")\n",
    "        except Exception as e:\n",
    "            print(f\" - Error processing document: {str(e)}\")\n",
    "    \n",
    "    # Get updated statistics\n",
    "    stats = embeddings_manager.get_statistics()\n",
    "    print(\"\\nUpdated Embeddings Manager Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\" - {key}: {value}\")\n",
    "else:\n",
    "    print(\"No processed documents available to add to vector store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Search Functionality with Real Documents\n",
    "\n",
    "Now let's test the search functionality with our real BCG sustainability report documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define some test queries related to BCG sustainability reports\n",
    "test_queries = [\n",
    "    \"What are BCG's carbon emission reduction targets?\",\n",
    "    \"How does BCG approach diversity and inclusion?\",\n",
    "    \"What is BCG's strategy for sustainable operations?\",\n",
    "    \"What community engagement initiatives does BCG participate in?\",\n",
    "    \"How is BCG supporting climate action with its clients?\"\n",
    "]\n",
    "\n",
    "# Test each query\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    # Search with MMR for diverse results\n",
    "    results = embeddings_manager.search(query, k=3, use_mmr=True)\n",
    "    \n",
    "    print(f\"Top {len(results)} results:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}\")\n",
    "        print(f\"Document: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:300]}...\" if len(doc.page_content) > 300 else f\"Content: {doc.page_content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Filter Search Results by Year or Document\n",
    "\n",
    "Let's test filtering search results by specific metadata attributes like year or document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test query with filters\n",
    "query = \"BCG's climate commitments\"\n",
    "\n",
    "# Filter by document\n",
    "if document_info and len(document_info) >= 2:\n",
    "    # Get document IDs for filtering\n",
    "    doc_id_1 = document_info[0]['document_id']\n",
    "    doc_id_2 = document_info[1]['document_id'] if len(document_info) > 1 else None\n",
    "    \n",
    "    if doc_id_1:\n",
    "        print(f\"\\nQuery: '{query}' filtered to document: {doc_id_1}\")\n",
    "        \n",
    "        # Create filter for specific document\n",
    "        doc_filter = {\"document_id\": doc_id_1}\n",
    "        \n",
    "        # Search with filter\n",
    "        filtered_results = embeddings_manager.search(query, k=2, filter=doc_filter, use_mmr=True)\n",
    "        \n",
    "        print(f\"Top {len(filtered_results)} results from document {doc_id_1}:\")\n",
    "        for i, doc in enumerate(filtered_results):\n",
    "            print(f\"\\nResult {i+1}\")\n",
    "            print(f\"Document: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "            print(f\"Content: {doc.page_content[:300]}...\" if len(doc.page_content) > 300 else f\"Content: {doc.page_content}\")\n",
    "    \n",
    "    if doc_id_2:\n",
    "        print(f\"\\nQuery: '{query}' filtered to document: {doc_id_2}\")\n",
    "        \n",
    "        # Create filter for specific document\n",
    "        doc_filter = {\"document_id\": doc_id_2}\n",
    "        \n",
    "        # Search with filter\n",
    "        filtered_results = embeddings_manager.search(query, k=2, filter=doc_filter, use_mmr=True)\n",
    "        \n",
    "        print(f\"Top {len(filtered_results)} results from document {doc_id_2}:\")\n",
    "        for i, doc in enumerate(filtered_results):\n",
    "            print(f\"\\nResult {i+1}\")\n",
    "            print(f\"Document: {doc.metadata.get('filename', 'Unknown')}\")\n",
    "            print(f\"Content: {doc.page_content[:300]}...\" if len(doc.page_content) > 300 else f\"Content: {doc.page_content}\")\n",
    "else:\n",
    "    print(\"Not enough processed documents available for document filtering test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Results Across Reports\n",
    "\n",
    "Let's compare results across different BCG sustainability reports to see how BCG's approach has evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a query on a topic that might have evolved over time\n",
    "evolution_query = \"BCG's approach to diversity, equity, and inclusion\"\n",
    "\n",
    "# Check if we have multiple documents\n",
    "if document_info and len(document_info) >= 2:\n",
    "    # Get document IDs and filenames\n",
    "    doc_ids = [doc['document_id'] for doc in document_info]\n",
    "    doc_filenames = [doc['filename'] for doc in document_info]\n",
    "    \n",
    "    # Create a DataFrame to store the comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    print(f\"Comparing results for query: '{evolution_query}' across documents\")\n",
    "    \n",
    "    # Search each document separately\n",
    "    for doc_id, filename in zip(doc_ids, doc_filenames):\n",
    "        # Create filter for specific document\n",
    "        doc_filter = {\"document_id\": doc_id}\n",
    "        \n",
    "        # Search with filter\n",
    "        results = embeddings_manager.search(evolution_query, k=1, filter=doc_filter, use_mmr=False)\n",
    "        \n",
    "        if results:\n",
    "            result_doc = results[0]\n",
    "            comparison_data.append({\n",
    "                \"Document\": filename,\n",
    "                \"Content\": result_doc.page_content[:500] + (\"...\" if len(result_doc.page_content) > 500 else \"\"),\n",
    "            })\n",
    "    \n",
    "    # Create and display DataFrame\n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        display(comparison_df)\n",
    "    else:\n",
    "        print(\"No results found for comparison.\")\n",
    "else:\n",
    "    print(\"Not enough processed documents available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've successfully demonstrated the embeddings and vector store components of the BCG Multi-Agent & Multimodal AI Platform. We've generated embeddings for the processed BCG Sustainability Reports and stored them in a vector database that enables efficient semantic search.\n",
    "\n",
    "Key accomplishments include:\n",
    "1. Generating embeddings using a local sentence transformer model\n",
    "2. Creating a vector store for efficient similarity search\n",
    "3. Adding processed document chunks to the vector store\n",
    "4. Performing semantic searches with various queries\n",
    "5. Filtering results by document and metadata\n",
    "6. Comparing content across different documents\n",
    "\n",
    "These components form the foundation of our RAG system, which will be integrated into the multi-agent architecture in the next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}